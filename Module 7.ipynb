{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Module 7 - Solutions & Notes # "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 1. Types of dirty data and their downstream effects (N/A)##\n",
    "No Jupyter Notebook exercises for this section."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 2. Identifying Dirty Data ##"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Inspecting the Data ### \n",
    "In this section we will use the pandas.describe method to undertake an inspection of our data. In this step we are trying to get a feel for your data. What does it look like and what does it represent? More importantly we need to try to understand whether we have good quality data or not. \n",
    "\n",
    "In this step we will use an authentic dataset: the Laurelhurst crime dataset. The Laurelhurst crime dataset contains listings of crime data reports from the Seattle Police Department: 2008 â€“ Present (see: https://data.seattle.gov/Public-Safety/Laurelhurst-Crime/9awu-7nc4). We have already downloaded a copy of this csv file for you which is at GitHub (see:https://github.com/sandy-75/COIT20280 ). Please use the copy that we have provided so that you can get the same results that we do.\n",
    "\n",
    "Note that for the following code to work you will have to make sure that the 'Laurelhurst_Crime.csv' file that you downloaded is in the same directory as your Jupyter Notebook. For example, if your Jupyter Notebook is at /projects/COIT20280/Modules/Module_7/Module_7.ipynb your downloaded file will have to also be at /projects/COIT20280/Modules/Module_7/Laurelhurst_Crime.csv\n",
    "\n",
    "\n",
    " "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "The code cell below simply loads the data from our csv file into a DataFrame called crime_data. We will use this DataFrame to inspect our data in the following sections."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the data in the csv file to a dataframe\n",
    "import pandas as pd\n",
    "\n",
    "crime_data = pd.read_csv('Laurelhurst_crime.csv')"
   ]
  },
  {
   "source": [
    "Before we do any inspecting let's just do a bit of an error check to make sure that what we have in our dataframe adds up with the description given by the data providers (see: https://data.seattle.gov/Public-Safety/Laurelhurst-Crime/9awu-7nc4) and something hasn't gone wrong along the way. We can check the number of rows and columns with pandas shape method (see https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.shape.html) for more information. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Call the shape method now on your dataframe to see how many rows and columns you have.  "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "In the above you should see that there are 3990 rows and 17 columns. This adds up with the description given by the data provider at the time of downloading the csv. Note that if you used a more recent version of the crime dataset the row number given by shape may vary because this dataset is updated regularly. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "So now let's get a feel for what the data looks like - a manual inspection. Use the head() method (see https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.head.html) on the crime_data dataframe to see what the first 5 rows of data look like. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "Note that the head() method also provides information to us on what our columns are called which also adds up with the data description provided by the data providers. You'll also note the NaN in the last row of the Offense End DateTime. Pandas uses this sentinel value NaN (Not a Number) to indicate that there is a missing value here. We will look more at this in the next section. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Data Profiling ###"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Now we know something about what the data looks like at a low level let's do some data profiling. We are going to Use the describe method here to do so. See https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.describe.html for more information. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Numerical Data ###"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Use the describe method on the crime_data dataframe to return summary statistics about the numeric columns only. \n",
    "Hint: this is the default behaviour of describe"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "In the above you should get back a summary of three columns: Offence ID, Longitude and Latitude. One thing you will notice almost immediately is that the first column 'Offense ID' seems a little strange in the way it is displayed. When you used the head method previously on the dataframe you would have noticed that the Offense ID is a number that looks like 12601509306 for example. However, in the display above it will be displayed in e-notation as 2.540186e+10. The display makes it difficult to understand our summary statistics for the 'Offense ID' column. Let's fix this up before we go any further."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "There several ways to fix the display so that we can see our Offense ID properly. Perhaps the most straightforward though is to change the display options to display float values as suggested in this Stackoverflow post (https://stackoverflow.com/questions/38689125/how-to-get-rid-of-pandas-converting-large-numbers-in-excel-sheet-to-exponential).\n",
    "\n",
    "Do this now in the code cell below so we aren't using e-notation - to 2 decimal places is fine for the moment."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "Now check that you have indeed changed the display by using the describe method again on your crime_data dataframe"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "Ok, so now we've got rid of the annoying formatting we can try to take a better look at our data. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Inspecting each data column of the dataframe ###"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Now it's time to see what the summary statistics reveal about each data column. Does there seem to be anything amiss here? Let's walk through it...\n",
    "\n",
    "1. count - here we have the same count for each of the Offense ID, Longitude and Latitude. Count gives us a count of all non-null values. We know from our csv file import that there are 3990 rows. Our count values here are 3990 also which indicates we have no missing or null values in these columns. \n",
    "\n",
    "2. mean - mean is the average of all the values in the columns. The 'Offense ID' column is simply a unique ID numeric value so the mean reported by describe doesn't tell us a lot here. This is a different story for Longitude and Latitude though. We see our mean values are 47.66 for Latitude and -122.28 for Longitude. We need to ask ourselves whether these are sensible values. Firstly, we can find out valid values for Latitude and Longitude by a simple web search (if we don't know already). Valid values for Latitude in degress are between -90 and +90 (Southern and Northern hemispheres respectively) and valid values for Longitude are between -180 and +180 (coordinates west and east of the Meridean). Our average values fall within this range so they are at least valid. In addition we can type these Latitude and Longitude values directly into Google search (type 47.66,-122.28 into the search bar) and we are provided with a map and our exact coordinates which puts us directly in Laurelhurst - right under the Seattle Children's Hospital. So not only are the average values of the Latitude and Longitude valid they also seem sensible given this crime reporting is for the area of Laurelhurst. We aren't getting results that put us in the middle of the Pacific Ocean which may make us question the quality of our data.\n",
    "\n",
    "3. std - standard deviation is a measure of the variance of our data. For Offense ID this makes sense - we have a range of different values that are indeed quite large numbers so our std is itself quite large. Now what about Latitude and Longitude? We see that our Longitude varies by a small amount: 0.01. This seems small - which we would expect given that it is the crime reporting for a single residential neighbourhood - but not particularly accurate or precise. Our Latitude is given as 0.00 which seems very strange and indeed incorrect. We would expect our values to be small but we expect some variation in our Latitude. So is something wrong with our data or perhaps the precision with which our data is being displayed here? Well we know from our min and max reported by describe that we certainly don't have uniform data here. For std to accurately report 0 (zero) there could be no variation in our data at all. The problem is similar to the display issue we faced with 'Offense ID'. This time however, it is because we have a small number. In our initial display the default was 2 decimal places and when we changed the display to get rid of our e-notation it did nothing to change this. Let's change this now to get it to display to 10 decimal places with pd.options.display.float_format."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "Use the describe method again on your crime_date dataframe to see if it worked"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "3. (cont.) so now we have adjusted the display we can see that our standard deviation is now looking a lot better. The standard deviation is small but not zero. The quality of our underlying data at this level seems fine. \n",
    "\n",
    "4. min & max - The minimum and maximum values of the 'Offense ID' seem sensible. Note that without having more information on what is considered a valid range for this data the best we can do is rely on our knowledge of unique ID's (see for example https://en.wikipedia.org/wiki/Unique_identifier). At this point we can see that the max and min are whole numbers ranging from 7625486301 to 25401859485 - perhaps serially generated. There is nothing terribly untoward about this and it seems to comply with our knowledge of what a unique ID might look like. Minimum values of Latitude and Longitude also seem sensible. We apply the same logic as we did in Step 2 above to arrive at this determination. Confirm this to yourself by putting the minimum and maximum values of Latitude and Longitude into google and you will get the two boundary points (extreme lower left - which places you right in the middle of Union Bay - and extreme upper right - on 55th Avenue NE just north of Windemere Park) of all your data points. Note that while we don't know the exact boundaries of the area that is being reported on we can do some work ourselves to see whether the data makes sense. We know that all of the data in the Longitude and Latitude columns are at least valid values. Additionally, in the context of the reporting it seems to make sense. All of the crimes that are reported here are contained within two extreme Latitude and Longitude points that cover the residential neighbourhood of Laurelhurst and do not seem to be in areas that extend beyond this.\n",
    "\n",
    "5. percentiles (25%, 50%, 75%) - our percentiles simply tell us how much of the data is less than this value. So for 25% and 'Offense ID' it is stating here that 25% of the data is less than 7651776285. Likewise for Longitude 25% of the data is less than the value -122.2853526600 and for Latitude 25% of the data is less than the value 47.6593671100. It is the same for the 50% and 75%. At this stage this information simply gives us a feel for our data. It is an analysis of sorts and in terms of data cleaning there is not a lot to make us suspicious of the quality of our data. There is a reasonable spread in terms of the numbers being reported here. Again, without being an expert in the particular area of the data itself and without more powerful visualisation tools we can't know for sure - but at this early stage the data at least seems to make sense. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Non-numerical Data ###"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "So now that we have done the process for the numeric columns: 'Offense ID', 'Latitude' and 'Longitude' - let's do the same for our non-numeric columns. We want to exclude all non-numeric types so let's do this with the exclude parameter of the pandas describe method. See https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.describe.html to see how to exclude numeric types. \n",
    "\n",
    "Hint: make sure to import appropriately so you do not end up with a NameError"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "When we look at what describe has reported here on our non-numerical data we don't have any of the annoying issues with the display that we had with the numeric types. We can proceed with inspecting each data column of the dataframe straight away."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Inspecting each data column of the dataframe ###"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "1. count - remember that count gives us a count of all non-null values in each of the columns. Here we have the same count for 'Report Number', 'Report DateTime', 'Group A B', 'Crime Against Category', 'Offense Parent Group', 'Offense', 'Offense Code', 'Precinct', 'Sector', 'Beat', 'MCPP' and '100 Block Address' that we did for our numerical columns: 'Offense ID', 'Longitude' and 'Latitude'. That is, 3990. This tells us that there are no null values in these columns, which means there are no missing values we have to contend with. The same cannot be said for 'Offense Start DateTime' or 'Offense End DateTime'. Describe tells us here that there are null values in these columns that we will have to contend with. Given this information we should ask ourselves whether there is anything here which might make us suspicious of the underlying data quality. The only issue here is the 'Offense Start DateTime' and 'Offense End DateTime' but this fact doesn't seem terribly untoward. It would seem logical that not all reports would have an EndDate. Forexample - they might be still being investigated and in terms of the 'Start DateTime' this may simply be unknown. It would be something to double check if you had access to this information. At this stage we don't and we will have to be comfortable with this judgement call and make a note of it in our data cleaning report.   \n",
    "\n",
    "2. unique - this is a rather handy statistic for us both in terms of reporting categorical data and also numeric data (which the describe method doesn't do by default) in that it can give us an indication of duplicate data. Here we can see that for all of our non-numeric data we have repeats in the data. For example, let's look at Report Number. This has a unique value of 3817. However, we have 3990 rows. This means that there are some repeats in the values. The same goes for all of the other columns. This means that there is a potential for row duplication. If we had the Report Number giving a unique value of 3990 we would know that there is no row duplication and hence no duplication in our data. This is something to flag and investigate further on. What about the other data columns. What about Offense Start DateTime and Offense End DateTime? These not only have missing values as revealed by count but they also have repeats. Again, we don't know for sure whether this is suspicious or not without knowing more about the business context. One can think of numerous reasons why there may be Offenses that have the same start and end times. There may be peak times for crimes - crimes may genuinely occur at the same time and finish at the same time. When reporting crimes the accuracy and precision may be somewhat limited as well for whatever reason. At this stage, we could assume (and report this assumption) that these duplications are not untoward. The same goes for the Report DateTime. Group A B is a data category that works well with unique. If we had anything other than 2 here we might be suspicious of the quality of the data. While the metadata information provided by the Seattle Police Department doesn't give us a huge amount of detail we could assume that this category should only have either A or B. If there were anything greater than 2 reported here it would be worth noting and investigating further. All of the other categories here seem to make sense given the metadata information provided. Therefore, after looking at the statistics for unique all of the non-numeric columns here seem to make sense apart from the fact that we will have to check for duplicates later.  \n",
    "\n",
    "3. top & freq- together these two reported statistics tell us more about repeating values. It tells us the value that occurs the most and how many times it occurs. Here we can see that the Report Number 2018-162496 was the most repeated value occuring 5 times in our data. The other values that are reported here for the different columns provide us with some early analytical information. They are interesting at this stage but remember that our primary purpose now is to make sure we have quality data."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Detailed Inspection ####\n",
    "Let's investigate our potential duplication issue that we flagged in the previous step. We want to find out whether we have any duplicated data. An easy way to do this is through Pandas Dataframe.duplicated() method. See the docs (https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.duplicated.html) on this method. Put the results into another dataframe as described in this tutorial (https://thispointer.com/pandas-find-duplicate-rows-in-a-dataframe-based-on-all-or-selected-columns-using-dataframe-duplicated-in-python/) and print the results to the screen."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "The results of the above should reveal that while we had our suspicions of duplicated data this does not in fact appear to be an issue. It returns an empty dataframe. But before we get too far ahead of ourselves let's just make sure that this method is indeed working as we think it should. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Let's start by copying our crime_data DataFrame to another dataframe we can experiment with.\n",
    "\n",
    "Hint: this is a simple 1 line piece of code in the form df_copy = df"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "crime_data_copy = crime_data"
   ]
  },
  {
   "source": [
    "Now let's select a subset of data from our crime_data and put that in a new dataframe. Call this new dataframe what you want but make it a sensible name that is descriptive of the data. Use the iloc method to select the first 5 rows of data in the crime_data dataframe. See the pandas docs for more on how to use iloc to get a subset of data (https://pandas.pydata.org/pandas-docs/stable/getting_started/intro_tutorials/03_subset_data.html)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "Let's print this subset dataframe to the screen to make sure we've got everything right and selected the first five rows of data "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "Now that we have copied some data let's append that to our copied crime_data Dataframe which we set up to experiment with. Let's use pandas append method to do so. See the pandas documentation here for how to do so (see https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.append.html)\n",
    "\n",
    "Hint: Note that the result of applying append is to return a new DataFrame object. It does not change the original DataFrame to which you appended. Hence, you will need to store that new DataFrame object somewhere if you are going to use it in the future (which you will need to in the steps below). "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "Now we should check that our append has actually worked as we intended and quick print to screen should confirm this.  "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "In the above you should check that you have an extra 5 rows. You should also see here that you do indeed have repeats. Printing to screen will print the first 5 rows - which were the rows we copied - and the last 5 rows - which were the copied rows that we appended. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Now let's apply the duplicate method again on the dataframe that contains the duplicated data to see whether it works as we think it should. As you did previously put these results into a new dataframe and print this dataframe to the screen. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "In the above we should see those same 5 data rows that we copied earlier. If you don't have this retrace your steps and make sure you have done everything right (particularly by referring to the docs)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Summary ####\n",
    "In summary this section 2 has walked you through the inspection step of Elgabry's (2019) workflow for data cleaning (see https://towardsdatascience.com/the-ultimate-guide-to-data-cleaning-3969843991d4) on an authentic dataset. This was the Laurelhurst crime data set - an open data set provided by the Seattle Police Department. The next section will go into more detail on the cleaning methods available to us by continuing to follow through with this same example. This time however, the dataset has been changed to reflect a dirty dataset.  "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 3. Methods for Cleaning ##\n",
    "\n",
    "In this section we are going to look in more detail at the three different classes of dirty data and the methods available for cleaning them. We will be using the methods as presented by McKinney (2017) for doing so. Additionally, this time we will be using the unclean version of the Laurelhurst_crime.csv dataset: Laurelhurst_crime_unclean.csv. This csv file is also available on Github (see: ) and has been intentionally modified from the clean version to give you experience with cleaning data. Of course this seems a little around the wrong way. In practice you would be given an unclean dataset - or at least a dataset that is in a state which is unknown and so is assumed to require some level of cleaning. You would then work toward getting a clean dataset. However, for the purposes of teaching and practice going the wrong way around - seeing a clean data set and then seeing a dirty data set - will serve us well enough.        "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "As in Section 2 make sure that the 'Laurelhurst_crime_unclean.csv' file that you downloaded is in the same directory as your Jupyter Notebook. \n",
    "\n",
    "The code cell below simply loads the data from our csv file into a DataFrame called crime_data_unclean. We will use this DataFrame to inspect our data in the following sections."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "      Unnamed: 0 Report Number   Offense ID  Offense Start DateTime  \\\n",
       "0              0   2020-040665  12601509306  02/01/2020 08:00:00 PM   \n",
       "1              1   2020-035892  12498858259  01/29/2020 03:40:00 PM   \n",
       "2              2   2020-034200  12414526693  11/25/2019 08:00:00 AM   \n",
       "3              3   2020-901071  12220089092  01/22/2020 08:30:00 AM   \n",
       "4              4   2020-025006  12183510818  01/21/2020 04:19:00 PM   \n",
       "...          ...           ...          ...                     ...   \n",
       "3990           0   2020-040665  12601509306  02/01/2020 08:00:00 PM   \n",
       "3991           1   2020-035892  12498858259  01/29/2020 03:40:00 PM   \n",
       "3992           2   2020-034200  12414526693  11/25/2019 08:00:00 AM   \n",
       "3993           3   2020-901071  12220089092  01/22/2020 08:30:00 AM   \n",
       "3994           4   2020-025006  12183510818  01/21/2020 04:19:00 PM   \n",
       "\n",
       "        Offense End DateTime         Report DateTime Group A B  \\\n",
       "0     02/02/2020 08:00:00 AM  02/02/2020 11:12:21 AM         A   \n",
       "1     01/29/2020 04:00:00 PM  01/29/2020 08:56:31 PM         A   \n",
       "2     11/25/2019 08:20:00 AM  01/28/2020 06:26:21 PM         A   \n",
       "3     01/22/2020 08:30:00 AM  01/23/2020 11:32:07 AM         A   \n",
       "4                        NaN  01/21/2020 06:30:43 PM         A   \n",
       "...                      ...                     ...       ...   \n",
       "3990  02/02/2020 08:00:00 AM  02/02/2020 11:12:21 AM         A   \n",
       "3991  01/29/2020 04:00:00 PM  01/29/2020 08:56:31 PM         A   \n",
       "3992  11/25/2019 08:20:00 AM  01/28/2020 06:26:21 PM         A   \n",
       "3993  01/22/2020 08:30:00 AM  01/23/2020 11:32:07 AM         A   \n",
       "3994                     NaN  01/21/2020 06:30:43 PM         A   \n",
       "\n",
       "     Crime Against Category Offense Parent Group                   Offense  \\\n",
       "0                  PROPERTY  MOTOR VEHICLE THEFT       Motor Vehicle Theft   \n",
       "1                  PROPERTY        LARCENY-THEFT  Theft From Motor Vehicle   \n",
       "2                  PROPERTY        LARCENY-THEFT       Theft From Building   \n",
       "3                  PROPERTY        LARCENY-THEFT  Theft From Motor Vehicle   \n",
       "4                  PROPERTY        LARCENY-THEFT       Theft From Building   \n",
       "...                     ...                  ...                       ...   \n",
       "3990               PROPERTY  MOTOR VEHICLE THEFT       Motor Vehicle Theft   \n",
       "3991               PROPERTY        LARCENY-THEFT  Theft From Motor Vehicle   \n",
       "3992               PROPERTY        LARCENY-THEFT       Theft From Building   \n",
       "3993               PROPERTY        LARCENY-THEFT  Theft From Motor Vehicle   \n",
       "3994               PROPERTY        LARCENY-THEFT       Theft From Building   \n",
       "\n",
       "     Offense Code Precinct Sector Beat       MCPP  \\\n",
       "0             240        N      U   U3  SANDPOINT   \n",
       "1             23F        N      U   U3  SANDPOINT   \n",
       "2             23D        N      U   U3  SANDPOINT   \n",
       "3             23F        N      U   U3  SANDPOINT   \n",
       "4             23D        N      U   U3  SANDPOINT   \n",
       "...           ...      ...    ...  ...        ...   \n",
       "3990          240        N      U   U3  SANDPOINT   \n",
       "3991          23F        N      U   U3  SANDPOINT   \n",
       "3992          23D        N      U   U3  SANDPOINT   \n",
       "3993          23F        N      U   U3  SANDPOINT   \n",
       "3994          23D        N      U   U3  SANDPOINT   \n",
       "\n",
       "                    100 Block Address       Longitude      Latitude  \n",
       "0            36XX BLOCK OF NE 42ND ST -122.2887605220 47.6591600400  \n",
       "1           41XX BLOCK OF 42ND AVE NE -122.2827475600 47.6598456900  \n",
       "2     48XX BLOCK OF SAND POINT WAY NE -122.2833627690 47.6643825700  \n",
       "3            49XX BLOCK OF NE 50TH ST -122.2743934530 47.6648924700  \n",
       "4           52XX BLOCK OF 40TH AVE NE -122.2848180340 47.6679900400  \n",
       "...                               ...             ...           ...  \n",
       "3990         36XX BLOCK OF NE 42ND ST -122.2887605220 47.6591600400  \n",
       "3991        41XX BLOCK OF 42ND AVE NE -122.2827475600 47.6598456900  \n",
       "3992  48XX BLOCK OF SAND POINT WAY NE -122.2833627690 47.6643825700  \n",
       "3993         49XX BLOCK OF NE 50TH ST -122.2743934530 47.6648924700  \n",
       "3994        52XX BLOCK OF 40TH AVE NE -122.2848180340 47.6679900400  \n",
       "\n",
       "[3995 rows x 18 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>Report Number</th>\n      <th>Offense ID</th>\n      <th>Offense Start DateTime</th>\n      <th>Offense End DateTime</th>\n      <th>Report DateTime</th>\n      <th>Group A B</th>\n      <th>Crime Against Category</th>\n      <th>Offense Parent Group</th>\n      <th>Offense</th>\n      <th>Offense Code</th>\n      <th>Precinct</th>\n      <th>Sector</th>\n      <th>Beat</th>\n      <th>MCPP</th>\n      <th>100 Block Address</th>\n      <th>Longitude</th>\n      <th>Latitude</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>2020-040665</td>\n      <td>12601509306</td>\n      <td>02/01/2020 08:00:00 PM</td>\n      <td>02/02/2020 08:00:00 AM</td>\n      <td>02/02/2020 11:12:21 AM</td>\n      <td>A</td>\n      <td>PROPERTY</td>\n      <td>MOTOR VEHICLE THEFT</td>\n      <td>Motor Vehicle Theft</td>\n      <td>240</td>\n      <td>N</td>\n      <td>U</td>\n      <td>U3</td>\n      <td>SANDPOINT</td>\n      <td>36XX BLOCK OF NE 42ND ST</td>\n      <td>-122.2887605220</td>\n      <td>47.6591600400</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>2020-035892</td>\n      <td>12498858259</td>\n      <td>01/29/2020 03:40:00 PM</td>\n      <td>01/29/2020 04:00:00 PM</td>\n      <td>01/29/2020 08:56:31 PM</td>\n      <td>A</td>\n      <td>PROPERTY</td>\n      <td>LARCENY-THEFT</td>\n      <td>Theft From Motor Vehicle</td>\n      <td>23F</td>\n      <td>N</td>\n      <td>U</td>\n      <td>U3</td>\n      <td>SANDPOINT</td>\n      <td>41XX BLOCK OF 42ND AVE NE</td>\n      <td>-122.2827475600</td>\n      <td>47.6598456900</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>2020-034200</td>\n      <td>12414526693</td>\n      <td>11/25/2019 08:00:00 AM</td>\n      <td>11/25/2019 08:20:00 AM</td>\n      <td>01/28/2020 06:26:21 PM</td>\n      <td>A</td>\n      <td>PROPERTY</td>\n      <td>LARCENY-THEFT</td>\n      <td>Theft From Building</td>\n      <td>23D</td>\n      <td>N</td>\n      <td>U</td>\n      <td>U3</td>\n      <td>SANDPOINT</td>\n      <td>48XX BLOCK OF SAND POINT WAY NE</td>\n      <td>-122.2833627690</td>\n      <td>47.6643825700</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>2020-901071</td>\n      <td>12220089092</td>\n      <td>01/22/2020 08:30:00 AM</td>\n      <td>01/22/2020 08:30:00 AM</td>\n      <td>01/23/2020 11:32:07 AM</td>\n      <td>A</td>\n      <td>PROPERTY</td>\n      <td>LARCENY-THEFT</td>\n      <td>Theft From Motor Vehicle</td>\n      <td>23F</td>\n      <td>N</td>\n      <td>U</td>\n      <td>U3</td>\n      <td>SANDPOINT</td>\n      <td>49XX BLOCK OF NE 50TH ST</td>\n      <td>-122.2743934530</td>\n      <td>47.6648924700</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>2020-025006</td>\n      <td>12183510818</td>\n      <td>01/21/2020 04:19:00 PM</td>\n      <td>NaN</td>\n      <td>01/21/2020 06:30:43 PM</td>\n      <td>A</td>\n      <td>PROPERTY</td>\n      <td>LARCENY-THEFT</td>\n      <td>Theft From Building</td>\n      <td>23D</td>\n      <td>N</td>\n      <td>U</td>\n      <td>U3</td>\n      <td>SANDPOINT</td>\n      <td>52XX BLOCK OF 40TH AVE NE</td>\n      <td>-122.2848180340</td>\n      <td>47.6679900400</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3990</th>\n      <td>0</td>\n      <td>2020-040665</td>\n      <td>12601509306</td>\n      <td>02/01/2020 08:00:00 PM</td>\n      <td>02/02/2020 08:00:00 AM</td>\n      <td>02/02/2020 11:12:21 AM</td>\n      <td>A</td>\n      <td>PROPERTY</td>\n      <td>MOTOR VEHICLE THEFT</td>\n      <td>Motor Vehicle Theft</td>\n      <td>240</td>\n      <td>N</td>\n      <td>U</td>\n      <td>U3</td>\n      <td>SANDPOINT</td>\n      <td>36XX BLOCK OF NE 42ND ST</td>\n      <td>-122.2887605220</td>\n      <td>47.6591600400</td>\n    </tr>\n    <tr>\n      <th>3991</th>\n      <td>1</td>\n      <td>2020-035892</td>\n      <td>12498858259</td>\n      <td>01/29/2020 03:40:00 PM</td>\n      <td>01/29/2020 04:00:00 PM</td>\n      <td>01/29/2020 08:56:31 PM</td>\n      <td>A</td>\n      <td>PROPERTY</td>\n      <td>LARCENY-THEFT</td>\n      <td>Theft From Motor Vehicle</td>\n      <td>23F</td>\n      <td>N</td>\n      <td>U</td>\n      <td>U3</td>\n      <td>SANDPOINT</td>\n      <td>41XX BLOCK OF 42ND AVE NE</td>\n      <td>-122.2827475600</td>\n      <td>47.6598456900</td>\n    </tr>\n    <tr>\n      <th>3992</th>\n      <td>2</td>\n      <td>2020-034200</td>\n      <td>12414526693</td>\n      <td>11/25/2019 08:00:00 AM</td>\n      <td>11/25/2019 08:20:00 AM</td>\n      <td>01/28/2020 06:26:21 PM</td>\n      <td>A</td>\n      <td>PROPERTY</td>\n      <td>LARCENY-THEFT</td>\n      <td>Theft From Building</td>\n      <td>23D</td>\n      <td>N</td>\n      <td>U</td>\n      <td>U3</td>\n      <td>SANDPOINT</td>\n      <td>48XX BLOCK OF SAND POINT WAY NE</td>\n      <td>-122.2833627690</td>\n      <td>47.6643825700</td>\n    </tr>\n    <tr>\n      <th>3993</th>\n      <td>3</td>\n      <td>2020-901071</td>\n      <td>12220089092</td>\n      <td>01/22/2020 08:30:00 AM</td>\n      <td>01/22/2020 08:30:00 AM</td>\n      <td>01/23/2020 11:32:07 AM</td>\n      <td>A</td>\n      <td>PROPERTY</td>\n      <td>LARCENY-THEFT</td>\n      <td>Theft From Motor Vehicle</td>\n      <td>23F</td>\n      <td>N</td>\n      <td>U</td>\n      <td>U3</td>\n      <td>SANDPOINT</td>\n      <td>49XX BLOCK OF NE 50TH ST</td>\n      <td>-122.2743934530</td>\n      <td>47.6648924700</td>\n    </tr>\n    <tr>\n      <th>3994</th>\n      <td>4</td>\n      <td>2020-025006</td>\n      <td>12183510818</td>\n      <td>01/21/2020 04:19:00 PM</td>\n      <td>NaN</td>\n      <td>01/21/2020 06:30:43 PM</td>\n      <td>A</td>\n      <td>PROPERTY</td>\n      <td>LARCENY-THEFT</td>\n      <td>Theft From Building</td>\n      <td>23D</td>\n      <td>N</td>\n      <td>U</td>\n      <td>U3</td>\n      <td>SANDPOINT</td>\n      <td>52XX BLOCK OF 40TH AVE NE</td>\n      <td>-122.2848180340</td>\n      <td>47.6679900400</td>\n    </tr>\n  </tbody>\n</table>\n<p>3995 rows Ã— 18 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 49
    }
   ],
   "source": [
    "#load the data in the csv file to a dataframe\n",
    "import pandas as pd\n",
    " \n",
    "\n",
    "crime_data_unclean = pd.read_csv('Laurelhurst_crime_unclean.csv')\n",
    "crime_data_unclean\n"
   ]
  },
  {
   "source": [
    "The above gives you a look at the data - the first five rows and the last five rows. At this stage it simply looks like our appended_crime_data dataframe that we had previously.\n",
    "\n",
    "Let's get a quick overview of this data by applying pandas describe as we did in the previous section. This time include all columns of your data so you can see everything together"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "### 3.1 Missing Data ###\n",
    "When we inspected our data in the previous section we found that we had some missing data for 'Offense Start DateTime' and 'Offense End DateTime'. Pandas represents this data as NaN which is what we saw when we applied the head() method to our dataframe. \n",
    "\n",
    "Now there are two main methods for dealing with missing data which we will cover below: filtering out data and filling in data.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Filtering Out Data ####\n",
    "If we were told that our analysis relied only on a complete data set - all records had to have no missing values for any of the columns - we would have to exclude these values.  In the code cell below go ahead and filter out all rows that contain missing values. \n",
    "\n",
    "Hint: McKinney (2017) shows us how this can be done in Section: 7.1 Handling Missing Data, Subsection: Filtering Out Missing Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "Now apply describe to do a data profile now that you don't have records with missing values. To keep this tidy apply describe for numerical values and then describe for the other values."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "#### Filling In Missing Data ####\n",
    "The other method we have available to us for dealing with missing data is filling in missing data.  \n",
    "\n",
    "Hint: McKinney (2017) shows us how this can be done in Section: 7.1 Handling Missing Data, Subsection: Filling In Missing Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "We have been told that our analysis must include all available information. One method for filling in data is to use a forward fill method. This method takes the last valid observation and forward fills this value until it gets to the next valid observation. See the docs for more information (https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.fillna.html). "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Apply this method now on the unclean data ie. the dataframe that represents Laurlehurst_data_unclean.csv "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "Let's compare what we have in the above printout with what we had when we first printed out our crime_data_unclean dataframe. We can see that the original NaN that we had appearing in the fifth and last rows for the column 'Offense End DateTime' has now been filled in. We can also see that this value is exactly the same as the value before it. \n",
    "\n",
    "Remember that fillna does not change the original dataframe and that if we wanted to use this dataframe going forward we should either put these results into a new dataframe or change the inplace parameter to 'True'. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### ??Bonus Marks for Random?? Challenge Question... ###\n",
    "\n",
    "Now let's look at filling our missing values with randomly generated values. This is a challenge question for you. You should use your readings from McKinney (2017), the official python and pandas documentation as well as postings from StackOverflow to do so. \n",
    "\n",
    "In terms of what is required for missing values that are numbers the randomly generated values should be between the minimum and maximum values that are in the clean data set. For non-numeric values they should be randomly generated from the list of all available options in the clean dataset."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "### 3.2 Surplus Data ###\n",
    "Our next class of dirty data is surplus data. Surplus data includes irrelevant data (unwanted observations, unwanted outliers) and duplicate data. We already went through identifying duplicate data in our previous section. Here we will cover how to deal with unwanted observations and unwanted outliers as well as duplicate data.  \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Irrelevant Data ####\n",
    "Remember that clean and unclean data is a matter of perspective. Data can be valid and without errors but if we don't want it in our analysis for whatever reason then we can consider it as unclean in that it is not fit for purpose. \n",
    "\n",
    "In this section we will look at restricting our data to only certain observations. \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Let's say that we have been told we have to do an analysis on crimes that occurred in an area encompassed by the 25th and 75th percentiles of our Latitude and Longitude (based on the clean data). Your job is to create a new dataframe that is restricted to all crimes reported in what represents this area.\n",
    "\n",
    "Hint: This post on stackoverflow provides a good walkthrough on how to select rows of a dataframe based on certain criteria of a column value (see: https://stackoverflow.com/questions/17071871/how-do-i-select-rows-from-a-dataframe-based-on-column-values). Make sure you put your selection into a new dataframe and for the purposes of this small exercise make your selection of rows from the original clean dataframe: crime_data.\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "Print this to screen to make sure that you have data in your dataframe"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "#### Duplicate Data ####\n",
    "Previously we looked at identifying duplicate data. Now it's time to remove that duplicate data. Follow McKinney (2017) and remove duplicate data from the dataframe crime_data_unclean. Put this in a new dataframe named appropriately."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Firstly, however, we should identify what duplicates we have in the dataframe like we did in Section 2 above. Do this in the code cell below "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "So now we know what data has been duplicated let's go ahead and remove the duplicate data from the dataframe crime_data_unclean. Store this in a new dataframe with a suitable name."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "Now, to see that this has worked let's see if we can pick up any duplicates.\n",
    "\n",
    "Hint: our dataframe should be empty if we print to screen"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "### 3.2 Unrefined Data ###\n",
    "Unrefined data includes data that contains structural errors (eg. syntax errors and type conversion errors) or is in an unstructured form. In this section we are going to go through a few of the common methods we have available to us for cleaning structural errors in data. In this section we will be using the crime_data_unclean dataframe.  "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Structural Errors ####\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "You may have noticed that when you applied describe() to the unclean dataset the results were quite different to the clean dataset. Just to make sure that you do notice the difference apply describe again to the two different datasets below."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "Firstly, let's start by looking at the 'Group A B' column. (more on this)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Go through the other columns..."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}